{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d5be46",
   "metadata": {},
   "source": [
    "# Face Vid2Vid Demo\n",
    "\n",
    "- Paper: One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing (CVPR 2021): [project](https://nvlabs.github.io/face-vid2vid/), [arxiv](https://arxiv.org/abs/2011.15126)\n",
    "- üë©üèª‚Äçüíª Developer : [Jihye Back](https://github.com/happy-jihye)\n",
    "\n",
    "This notebooks is an unofficial demo web app of the `face video2video`.\n",
    "\n",
    "The codes are heavily based on [this code, created by `zhanglonghao1992`](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis) (thank you!üòä).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f91001",
   "metadata": {},
   "source": [
    "# Gradio App\n",
    "\n",
    "## 1. import libraries & load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d1cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import imageio, cv2\n",
    "from moviepy.editor import *\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "\n",
    "from demo import load_checkpoints, make_animation, find_best_frame\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "config = 'config/vox-256-spade.yaml'\n",
    "checkpoint = 'ckpt/00000189-checkpoint.pth.tar'\n",
    "\n",
    "gen = 'spade'\n",
    "cpu = False\n",
    "\n",
    "generator, kp_detector, he_estimator = load_checkpoints(config_path=config, checkpoint_path=checkpoint, gen=gen, cpu=cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ba0e0",
   "metadata": {},
   "source": [
    "## inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba94ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(source,\n",
    "              driving,\n",
    "              find_best_frame_ = False,\n",
    "              free_view = False,\n",
    "              yaw = None,\n",
    "              pitch = None,\n",
    "              roll = None,\n",
    "              output_name = 'output.mp4',\n",
    "              \n",
    "              audio = True,\n",
    "              cpu = False,\n",
    "              best_frame = None,\n",
    "              relative = True,\n",
    "              adapt_scale = True,\n",
    "              ):\n",
    "\n",
    "    # source \n",
    "    source_image = resize(source, (256, 256))\n",
    "    \n",
    "    # driving\n",
    "    reader = imageio.get_reader(driving)\n",
    "    fps = reader.get_meta_data()['fps']\n",
    "    driving_video = []\n",
    "    try:\n",
    "        for im in reader:\n",
    "            driving_video.append(im)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    reader.close()\n",
    "\n",
    "    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
    "    \n",
    "    with open(config) as f:\n",
    "        config_ = yaml.load(f)\n",
    "    estimate_jacobian = config_['model_params']['common_params']['estimate_jacobian']\n",
    "    print(f'estimate jacobian: {estimate_jacobian}')\n",
    "\n",
    "    if find_best_frame_ or best_frame is not None:\n",
    "        i = best_frame if best_frame is not None else find_best_frame(source_image, driving_video, cpu=cpu)\n",
    "        print (\"Best frame: \" + str(i))\n",
    "        driving_forward = driving_video[i:]\n",
    "        driving_backward = driving_video[:(i+1)][::-1]\n",
    "        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
    "    else:\n",
    "        predictions = make_animation(source_image, driving_video, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "    \n",
    "    # save video\n",
    "    output_path = 'asset/output'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    print(f'{output_path}/{output_name}')    \n",
    "    \n",
    "    imageio.mimsave(f'{output_path}/{output_name}', [img_as_ubyte(frame) for frame in predictions], fps=fps)\n",
    "    \n",
    "    if audio:\n",
    "        audioclip = VideoFileClip(driving)\n",
    "        audio = audioclip.audio\n",
    "        videoclip = VideoFileClip(f'{output_path}/{output_name}')\n",
    "        videoclip.audio = audio\n",
    "        name = output_name.strip('.mp4')\n",
    "        videoclip.write_videofile(f'{output_path}/{name}_audio.mp4')\n",
    "        return f'./{output_path}/{name}_audio.mp4'\n",
    "    else:\n",
    "        return f'{output_path}/{output_name}'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5b017",
   "metadata": {},
   "source": [
    "## 2. run gradio app ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64fbc64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecationWarning",
     "evalue": "The `server_name` and `server_port` parameters in `Interface`are deprecated. Please pass into launch() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecationWarning\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7297/2673922909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mserver_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0.0.0.0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     server_port = 8889)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/vid2vid/lib/python3.7/site-packages/gradio/interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, inputs, outputs, verbose, examples, examples_per_page, live, layout, show_input, show_output, capture_session, interpretation, num_shap, theme, repeat_outputs_per_model, title, description, article, thumbnail, css, height, width, allow_screenshot, allow_flagging, flagging_options, encrypt, show_tips, flagging_dir, analytics_enabled, server_name, server_port, enable_queue, api_mode, flagging_callback)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mserver_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mserver_port\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise DeprecationWarning(\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;34m\"The `server_name` and `server_port` parameters in `Interface`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0;34m\"are deprecated. Please pass into launch() instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             )\n",
      "\u001b[0;31mDeprecationWarning\u001b[0m: The `server_name` and `server_port` parameters in `Interface`are deprecated. Please pass into launch() instead."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "samples = []\n",
    "\n",
    "example_source = os.listdir('asset/source')\n",
    "for image in example_source:\n",
    "    samples.append([f'asset/source/{image}', None])\n",
    "\n",
    "example_driving = os.listdir('asset/driving')\n",
    "for video in example_driving:\n",
    "    samples.append([None, f'asset/driving/{video}'])\n",
    "\n",
    "iface = gr.Interface(\n",
    "    inference, # main function\n",
    "    inputs = [ \n",
    "        gr.inputs.Image(shape=(255, 255), label='Source Image'), # source image\n",
    "        gr.inputs.Video(label='Driving Video', type='mp4'), # driving video\n",
    "        \n",
    "        gr.inputs.Checkbox(label=\"fine best frame\", default=False), \n",
    "        gr.inputs.Checkbox(label=\"free view\", default=False), \n",
    "        gr.inputs.Slider(minimum=-90, maximum=90, default=0, step=1, label=\"yaw\"),\n",
    "        gr.inputs.Slider(minimum=-90, maximum=90, default=0, step=1, label=\"pitch\"),\n",
    "        gr.inputs.Slider(minimum=-90, maximum=90, default=0, step=1, label=\"raw\"),\n",
    "        \n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.outputs.Video(label='result') # generated video\n",
    "    ], \n",
    "    \n",
    "    title = 'Face Vid2Vid Demo',\n",
    "    description = \"This app is an unofficial demo web app of the face video2video. The codes are heavily based on this repo, created by zhanglonghao1992\",\n",
    "    \n",
    "    examples = samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f58ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7297/3656101434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'iface' is not defined"
     ]
    }
   ],
   "source": [
    "iface.launch(server_name = '0.0.0.0',\n",
    "    server_port = 8889)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef1aba",
   "metadata": {},
   "source": [
    "# Inference on notebook\n",
    "\n",
    "If you want to align the raw image, refer to [this repo](https://github.com/happy-jihye/FFHQ-Alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd764be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "# https://github.com/tg-bomze/Face-Image-Motion-Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "placeholder_bytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/x8AAwMCAO+ip1sAAAAASUVORK5CYII=')\n",
    "placeholder_image = imageio.imread(placeholder_bytes, '.png')\n",
    "placeholder_image = resize(placeholder_image, (256, 256))[..., :3]\n",
    "\n",
    "def display(source, driving, generated=None):\n",
    "    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n",
    "    ims = []\n",
    "    for i in range(len(driving)):\n",
    "        cols = [[placeholder_image], []]\n",
    "        for sourceitem in source:\n",
    "            cols[0].append(sourceitem)\n",
    "        cols[1].append(driving[i])\n",
    "        if generated is not None:\n",
    "            for generateditem in generated:\n",
    "                cols[1].append(generateditem[i])\n",
    "\n",
    "        endcols = []\n",
    "        for thiscol in cols:\n",
    "            endcols.append(np.concatenate(thiscol, axis=1))\n",
    "\n",
    "        im = plt.imshow(np.vstack(endcols), animated=True) # np.concatenate(cols[0], axis=1)\n",
    "        plt.axis('off')\n",
    "        ims.append([im])\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
    "    plt.close()\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b318c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [00:12, 21.33it/s]\n",
      "  0%|          | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best frame: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:10<00:00,  5.30it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 209/209 [00:39<00:00,  5.32it/s]\n",
      "265it [00:13, 19.72it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best frame: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:09<00:00,  5.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 215/215 [00:40<00:00,  5.30it/s]\n",
      "265it [00:12, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best frame: 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:15<00:00,  5.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:35<00:00,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "source_list = ['asset/source/3.png', 'asset/source/5.png', 'asset/source/4.png']\n",
    "driving = 'asset/driving/1.mp4'\n",
    "\n",
    "# saving path\n",
    "os.makedirs('asset/output', exist_ok=True)\n",
    "\n",
    "find_best_frame_ = True\n",
    "free_view = False\n",
    "yaw = None\n",
    "pitch = None\n",
    "roll = None\n",
    "\n",
    "cpu = False\n",
    "best_frame = None\n",
    "relative = True\n",
    "adapt_scale = True\n",
    "estimate_jacobian = False\n",
    "\n",
    "\n",
    "# driving\n",
    "reader = imageio.get_reader(driving)\n",
    "fps = reader.get_meta_data()['fps']\n",
    "driving_video = []\n",
    "try:\n",
    "    for im in reader:\n",
    "        driving_video.append(im)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "reader.close()\n",
    "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
    "\n",
    "source_images = []\n",
    "final = []\n",
    "for idx, source in enumerate(source_list):\n",
    "    # source \n",
    "    source_image = imageio.imread(source)[..., :3]\n",
    "    source_image = resize(source_image, (256, 256))\n",
    "    source_images.append(source_image)\n",
    "    \n",
    "    # inference\n",
    "    if find_best_frame_ or best_frame is not None:\n",
    "        i = best_frame if best_frame is not None else find_best_frame(source_image, driving_video, cpu=cpu)\n",
    "        print (\"Best frame: \" + str(i))\n",
    "        driving_forward = driving_video[i:]\n",
    "        driving_backward = driving_video[:(i+1)][::-1]\n",
    "        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
    "    else:\n",
    "        predictions = make_animation(source_image, driving_video, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "\n",
    "    imageio.mimsave(f'asset/output/{idx}.mp4', [img_as_ubyte(frame) for frame in predictions])\n",
    "    final.append(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b30edabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_video = display(source_images, driving_video, final)\n",
    "final_video.save(f'asset/output/final_1.mp4', fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe7c4644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"asset/output/final_1.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('asset/output/final_1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fd404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
